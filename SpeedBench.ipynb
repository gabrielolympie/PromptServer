{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1567893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from clients.prompt_client import PromptClient\n",
    "import json\n",
    "\n",
    "client = PromptClient(\n",
    "    base_url=f\"{os.getenv('BASE_URL')}/{os.getenv('APPLICATION_NAME')}/v1\",\n",
    "    api_key=os.getenv('API_TOKEN')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bdf5c0",
   "metadata": {},
   "source": [
    "## Of course, you can also stream the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6396ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "response = client.stream(\n",
    "    input_data={'topic': \"guacamole\"},\n",
    "    route=\"/prompts/speedbench\"\n",
    ")\n",
    "\n",
    "report = \"\"\n",
    "intervals = [64, 128, 256, 512, 1024, 2048]\n",
    "interval_index = 0\n",
    "t0 = None\n",
    "\n",
    "for i, elt in enumerate(response):\n",
    "\n",
    "\n",
    "    if elt['format'] == 'text':\n",
    "        print(elt['content'], end=\"\")\n",
    "        \n",
    "    if i == 0:\n",
    "        t0 = time.time()\n",
    "\n",
    "    # Check if we've reached the next interval\n",
    "    if interval_index < len(intervals) and i == intervals[interval_index]:\n",
    "        t1 = time.time()\n",
    "        delta = t1 - t0\n",
    "        tokens_in_interval = intervals[interval_index] - (intervals[interval_index - 1] if interval_index > 0 else 0)\n",
    "        rate = tokens_in_interval / delta\n",
    "        report += f\"{intervals[interval_index-1] if interval_index > 0 else 0}-{intervals[interval_index]}: {rate:.2f} tokens/sec\\n\"\n",
    "        t0 = t1  # Reset timer for next interval\n",
    "        interval_index += 1\n",
    "        \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fdae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "topics = [\n",
    "    \"guacamole\",\n",
    "    \"artificial intelligence\",\n",
    "    \"quantum computing\",\n",
    "    \"sustainable energy\",\n",
    "    \"space exploration\",\n",
    "    \"neuroscience\",\n",
    "    \"blockchain technology\",\n",
    "    \"virtual reality\",\n",
    "    \"climate change\",\n",
    "    \"biotechnology\",\n",
    "    \"robotics\",\n",
    "    \"cybersecurity\",\n",
    "    \"renewable resources\",\n",
    "    \"machine learning\",\n",
    "    \"genetic engineering\",\n",
    "    \"autonomous vehicles\"\n",
    "] * 8\n",
    "\n",
    "\n",
    "response = client.batch(\n",
    "    input_data=[{'topic': elt} for elt in topics],\n",
    "    route=\"/prompts/speedbench\",\n",
    "    n_jobs=32,\n",
    "    verbose=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt-oss-120b\n",
    "0-64: 182.14 tokens/sec\n",
    "64-128: 147.11 tokens/sec\n",
    "128-256: 158.66 tokens/sec\n",
    "256-512: 143.20 tokens/sec\n",
    "512-1024: 154.57 tokens/sec\n",
    "1024-2048: 148.10 tokens/sec\n",
    "\n",
    "batch_4: ~403-409 tokens/sec\n",
    "batch_8: ~770-776 tokens/sec\n",
    "batch_16: ~1294-1302 tokens/sec\n",
    "batch_32: ~1986-2146 tokens/sec\n",
    "\n",
    "\n",
    "gpt-oss-20b\n",
    "0-64: 196.09 tokens/sec\n",
    "64-128: 199.98 tokens/sec\n",
    "128-256: 214.26 tokens/sec\n",
    "256-512: 198.01 tokens/sec\n",
    "512-1024: 196.56 tokens/sec\n",
    "1024-2048: 194.38 tokens/sec\n",
    "\n",
    "batch_4: ~564-624 tokens/sec\n",
    "batch_8: ~1054-1117 tokens/sec\n",
    "batch_16: ~1887-1912 tokens/sec\n",
    "batch_32: ~2904-2911 tokens/sec\n",
    "\n",
    "\n",
    "Qwen3-32B-AWQ\n",
    "0-64: 60.47 tokens/sec\n",
    "64-128: 68.94 tokens/sec\n",
    "128-256: 62.53 tokens/sec\n",
    "256-512: 62.36 tokens/sec\n",
    "512-1024: 61.99 tokens/sec\n",
    "\n",
    "batch_4: ~227-233 tokens/sec\n",
    "batch_8: ~447-452 tokens/sec\n",
    "batch_16: ~920-936 tokens/sec\n",
    "batch_32: ~1448-1482 tokens/sec\n",
    "\n",
    "\n",
    "Mistral-Small-3.2-24B-Instruct-hf-AWQ\n",
    "0-64: 89.39 tokens/sec\n",
    "64-128: 95.77 tokens/sec\n",
    "128-256: 89.29 tokens/sec\n",
    "256-512: 87.29 tokens/sec\n",
    "512-1024: 86.95 tokens/sec\n",
    "1024-2048: 86.59 tokens/sec\n",
    "\n",
    "batch_4: ~288-336 tokens/sec\n",
    "batch_8: ~631-646 tokens/sec\n",
    "batch_16: ~1109-1153 tokens/sec\n",
    "batch_32: ~1714-1790 tokens/sec\n",
    "\n",
    "\n",
    "Qwen3-4B-Instruct-2507-GPTQ\n",
    "0-64: 208.21 tokens/sec\n",
    "64-128: 205.15 tokens/sec\n",
    "128-256: 223.60 tokens/sec\n",
    "256-512: 210.72 tokens/sec\n",
    "512-1024: 211.67 tokens/sec\n",
    "1024-2048: 207.49 tokens/sec\n",
    "\n",
    "batch_4: ~721-743 tokens/sec\n",
    "batch_8: ~1158-1377 tokens/sec\n",
    "batch_16: ~2044-2236 tokens/sec\n",
    "batch_32: ~2400-2666 tokens/sec\n",
    "\n",
    "\n",
    "Qwen3-Coder-30B-A3B-Instruct-GPTQ-4bit\n",
    "0-64: 179.42 tokens/sec\n",
    "64-128: 176.71 tokens/sec\n",
    "128-256: 176.01 tokens/sec\n",
    "256-512: 175.81 tokens/sec\n",
    "512-1024: 175.44 tokens/sec\n",
    "1024-2048: 172.64 tokens/sec\n",
    "\n",
    "batch_4: ~490-510 tokens/sec\n",
    "batch_8: ~950-1000 tokens/sec\n",
    "batch_16: ~1520-1602 tokens/sec\n",
    "batch_32: ~2200-2400 tokens/sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e46c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97e50f37",
   "metadata": {},
   "source": [
    "## Post\n",
    "\n",
    "# RTX PRO 6000 Blackwell for LLM\n",
    "\n",
    "Just received my brand new Blackwell card, so did a quick bench to let the community grasp the pros and cons\n",
    "\n",
    "## Setup Details:\n",
    "GPU : Rtx pro 6000 max-q workstation edition, 20% less performance than the complete, but with half the power draw. 2 slots\n",
    "CPU : Ryzen 9 3950X, 24 channels, 16 cores / 32 threads\n",
    "RAM : 128go DDR4 @33600\n",
    "\n",
    "GPU1 : RTX 3090 24gb blower edition. 2 slots, unused here\n",
    "GPU2 : RTX 3090 24gb founder edition. 3 slots, unused here\n",
    "\n",
    "## Software details\n",
    "### OS\n",
    "Ubuntu 22.04\n",
    "nvidia drivers : 770 open\n",
    "Cuda toolkit 13\n",
    "\n",
    "### Env\n",
    "conda create --name vllm python=3.12\n",
    "conda activate vllm\n",
    "\n",
    "uv pip install flashinfer-python --prerelease=allow --upgrade --extra-index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "uv pip install vllm --torch-backend=cu128\n",
    "\n",
    "## Training Benchmark\n",
    "Two stuff are diferenciating for training on that card:\n",
    "- the number of tensor core is outstanding, about 60% more than a single B100 gpu\n",
    "- the 96GB vram is a game changer for training, enabling very large batch, so faster and smoother training\n",
    "\n",
    "### Experiment:\n",
    "Pretraining of a SLM with 35M parameters, based on GQA architecture with 8 layers, trained with pytorch lightning.\n",
    "Training dataset is TinyStories, with a budget of 1B tokens (2 epochs), a sequence length of 256 tokens, and a virtual batch size of 100k tokens.\n",
    "Models are trained in mixed bf16 precision (additionnal improvement could be expected from using black well fp8 training)\n",
    "\n",
    "### Results:\n",
    "- 1 x 4090 Laptop (similar perf as a 3090 Desktop) : ~2.5 hours to complete the training run\n",
    "- 1 x RTX 6000 pro maxq workstation : ~20 min to complete the training run\n",
    "\n",
    "### Conclusion\n",
    "With proper optim, the card can single handedly deliver the training compute of 7.5 rtx 3090 card, while pulling only 300W of electricity (and being very quiet).\n",
    "\n",
    "## Inference Benchmark\n",
    "In inference, bandwith can be the bottleneck factor, especially in batch 1 inference.\n",
    "\n",
    "Let's assess the results in batch 1, 4, 8, 16 and 32 to see how much token we can squeeze out of the card.\n",
    "\n",
    "### Launch\n",
    "```bash\n",
    "export NVCC_THREADS=16\n",
    "export MAX_JOBS=16\n",
    "export OMP_NUM_THREADS=16\n",
    "export VLLM_ATTENTION_BACKEND=FLASHINFER\n",
    "export ENABLE_NVFP4_SM120=1\n",
    "export VLLM_USE_FLASHINFER_MOE_FP4=1\n",
    "export MODEL_NAME=\"DeepSeek-R1-0528-Qwen3-8B-FP4\"\n",
    "vllm serve \"$MODEL_NAME\" \\\n",
    "--served-model-name gpt-4 \\\n",
    "--port 5000 \\\n",
    "--max-model-len 16000 \\\n",
    "--gpu-memory-utilization 0.9 \\\n",
    "--trust_remote_code \\\n",
    "--max-seq-len-to-capture 8196 \\\n",
    "--enable-chunked-prefill  \\\n",
    "--kv-cache-dtype fp8 \\\n",
    "--compilation-config '{\"pass_config\":{\"enable_fusion\":true,\"enable_noop\":true},\"cudagraph_mode\":1,\"max_capture_size\":2048}'\n",
    "```\n",
    "\n",
    "### Launch >20B Active\n",
    "On larger models, tensor cores can do wonders, so above 20B active parameters, the following additionnal env variables can provide a small speed increase, especially for batching.\n",
    "export VLLM_USE_TRTLLM_ATTENTION=1\n",
    "export VLLM_USE_TRTLLM_FP4_GEMM=1\n",
    "export VLLM_FLASHINFER_FORCE_TENSOR_CORES=1\n",
    "\n",
    "Note: i ran every speed test without these flags, but for example Mistral Small would give around 95 t/s on batch 1, and 1950 t/s on batch 32\n",
    "\n",
    "### Launch QWEN Moe\n",
    "Add flag --enable-expert-parallel\n",
    "\n",
    "### Launch GPT-OSS\n",
    "GPT OSS relies on MXFP4 quant (cause why would they do like everyone else uh?), an hybrid format that will most likely disapear once NVFP4 is fully supported.\n",
    "They also are leveraging their own library for prompt formatting, that is not really compatible with vllm, so don't expect to get anything good from these, i am jsut testing the speed, but most of the time they only send you blank tokens, which is not really impressive.\n",
    "\n",
    "#### DOWNLOADS\n",
    "You'll need to download the following to make vllm work with special snowflake tokenizer, and not break on start:\n",
    "sudo wget -O /etc/encodings/o200k_base.tiktoken https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken\n",
    "sudo wget -O /etc/encodings/cl100k_base.tiktoken https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\n",
    "\n",
    "#### Launch Command\n",
    "```bash\n",
    "export ENABLE_NVFP4_SM120=1\n",
    "export VLLM_USE_TRTLLM_ATTENTION=1\n",
    "export OMP_NUM_THREADS=16\n",
    "export TIKTOKEN_ENCODINGS_BASE=/etc/encodings  \n",
    "export VLLM_USE_FLASHINFER_MXFP4_BF16_MOE=1 \n",
    "export VLLM_USE_FLASHINFER_MXFP4_MOE=1 \n",
    "export VLLM_ATTENTION_BACKEND=FLASHINFER\n",
    "export MODEL_NAME=\"gpt-oss-120b\"\n",
    "vllm serve \"$MODEL_NAME\" \\\n",
    "--async-scheduling \\\n",
    "--served-model-name gpt-4 \\\n",
    "--port 5000 \\\n",
    "--max-model-len 16000 \\\n",
    "--gpu-memory-utilization 0.9 \\\n",
    "--trust_remote_code \\\n",
    "--max-seq-len-to-capture 8196 \\\n",
    "--compilation-config '{\"pass_config\":{\"enable_fusion\":true,\"enable_noop\":true},\"cudagraph_mode\":1,\"max_capture_size\":2048}' \\\n",
    "```\n",
    "\n",
    "### Model Tested:\n",
    "- Qwen3-Coder-30B-A3B-Instruct-GPTQ-4bit\n",
    "- Qwen3-4B-Instruct-2507-GPTQ\n",
    "- Qwen3-32B-AWQ\n",
    "- Mistral-Small-3.2-24B-Instruct-hf-AWQ\n",
    "- gpt-oss-20b\n",
    "- gpt-oss-120b\n",
    "- Llama-4-Scout-17B-16E-Instruct-AWQ\n",
    "\n",
    "### Failed Test (where not able to run NVFP4 formats properly for some reasons)\n",
    "- DeepSeek-R1-0528-Qwen3-8B-FP4\n",
    "- Qwen3-32B-FP4\n",
    "\n",
    "### Results\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "No surprise, in batch 1, the performance is good but not outstanding, limited by the 1.7 TB/s of GDDR7 memory.\n",
    "The blackwell optimizations allow to squeeze a bit more performance though (that might explode when flash attention 4 will be released) and just slightly beats the speed of 2 x 3090 with tensor parallelism.\n",
    "\n",
    "The game changer is on batch 32, with an almost linear scaling of number of tokens delivered with batch size, so might be really usefull for small scale serving and multi agent deployment purpose.\n",
    "\n",
    "So far, support is still not completely ready, but sufficient to play with some models.\n",
    "\n",
    "## Code to reproduce the results\n",
    "Training scripts can be found on this repo for pretraining: https://github.com/gabrielolympie/ArchiFactory\n",
    "Speed Benchmark for inference + used prompts can be found in : https://github.com/gabrielolympie/PromptServer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d7783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptserver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
